import pandas as pd
from sklearn.decomposition import PCA

# Load the full embeddings file
df = pd.read_csv("tokenized_tweets_with_embeddings.csv", encoding="utf-8-sig")

# Check duplicates on the text column (assumed first column)
print(f"Original rows: {len(df)}")
print(f"Duplicates in text column: {df.iloc[:, 0].duplicated().sum()}")

# Drop duplicate tweets based on the text column (first column)
df_unique = df.drop_duplicates(subset=df.columns[0], keep='first').reset_index(drop=True)

print(f"Rows after dropping duplicates: {len(df_unique)}")  # Should be 2007

# Extract embeddings columns (all except the first text column)
embeddings = df_unique.iloc[:, 1:].values

# Apply PCA for dimensionality reduction
pca = PCA(n_components=50, random_state=42)
reduced = pca.fit_transform(embeddings)

# Create DataFrame for reduced embeddings with named columns
reduced_df = pd.DataFrame(reduced, columns=[f"PC{i+1}" for i in range(reduced.shape[1])])

# Combine reduced embeddings with unique tweets text column
reduced_combined = pd.concat([df_unique.iloc[:, [0]], reduced_df], axis=1)

# Save cleaned reduced embeddings CSV
reduced_combined.to_csv("reduced_embeddings.csv", index=False, encoding="utf-8-sig")

print("âœ… Saved cleaned reduced embeddings with shape:", reduced_combined.shape)
