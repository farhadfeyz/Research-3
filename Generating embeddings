import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm

# Load the ParsBERT model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("HooshvareLab/bert-base-parsbert-uncased")
model = AutoModel.from_pretrained("HooshvareLab/bert-base-parsbert-uncased")

# Load your cleaned tweets
df = pd.read_csv("Twitter_CLEANED.csv", encoding="utf-8-sig")  # UTF-8 with BOM
texts = df.iloc[:, 0].astype(str).tolist()  # Assumes the tweets are in the first column

# Function to get sentence embeddings
def get_bert_embeddings(texts, tokenizer, model):
    model.eval()
    embeddings = []
    
    with torch.no_grad():
        for text in tqdm(texts, desc="Embedding tweets"):
            inputs = tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=128
            )
            outputs = model(**inputs)
            last_hidden_state = outputs.last_hidden_state
            cls_embedding = last_hidden_state[:, 0, :]  # [CLS] token representation
            embeddings.append(cls_embedding.squeeze().numpy())
    
    return embeddings

# Get embeddings
embeddings = get_bert_embeddings(texts, tokenizer, model)

# Save the embeddings to a new DataFrame
embedding_df = pd.DataFrame(embeddings)
combined = pd.concat([df, embedding_df], axis=1)

# Save to CSV with UTF-8-BOM (to match input)
combined.to_csv("tokenized_tweets_with_embeddings.csv", index=False, encoding="utf-8-sig")
